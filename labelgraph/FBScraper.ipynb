{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "import uuid\n",
    "import itertools\n",
    "import urllib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import rdflib\n",
    "from rdflib import Namespace\n",
    "from rdflib.namespace import DC, DCTERMS, DOAP, FOAF, SKOS, OWL, RDF, RDFS, VOID, XMLNS, XSD\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node(node):\n",
    "    nodes.add(node)\n",
    "\n",
    "def read_nodes(path):\n",
    "    content = None\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "\n",
    "    for line in content:\n",
    "        head,rel,tail = line.split(\"\\t\")\n",
    "        add_node(head)\n",
    "        add_node(tail)\n",
    "        \n",
    "def divide_chunks(l, n):\n",
    "    for i in range(0, len(l), n): \n",
    "        yield l[i:i + n]\n",
    "        \n",
    "def load_json_file(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "def load_id_tsv(path):\n",
    "    content = None\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "\n",
    "    idValueDict = {}\n",
    "    for line in content:\n",
    "        id,value = line.split(\"\\t\")\n",
    "        idValueDict[id] = value\n",
    "    return idValueDict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14541\n"
     ]
    }
   ],
   "source": [
    "# https://developers.google.com/knowledge-graph/\n",
    "\n",
    "root = r\"workspace/labels\"\n",
    "train_path = r\"workspace\\data\\FB15-237\\train.txt\"\n",
    "test_path = r\"workspace\\data\\FB15-237\\test.txt\"\n",
    "valid_path = r\"workspace\\data\\FB15-237\\valid.txt\"\n",
    "\n",
    "nodes = set()\n",
    "read_nodes(train_path)\n",
    "read_nodes(test_path)\n",
    "read_nodes(valid_path)\n",
    "nodes = list(nodes)\n",
    "\n",
    "print(len(nodes))"
   ]
  },
  {
   "source": [
    "## Download files\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "30it [00:40,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "api_key = open('.api_key').read()\n",
    "\n",
    "service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "\n",
    "limit = 500\n",
    "\n",
    "result = {\n",
    "    'result': []\n",
    "}\n",
    "\n",
    "for node_list in tqdm(divide_chunks(nodes, limit)):\n",
    "    params = [\n",
    "        ('limit', limit),\n",
    "        ('indent', True),\n",
    "        ('key', api_key),\n",
    "    ]\n",
    "    params.extend(zip(itertools.repeat('ids'), node_list))\n",
    "    \n",
    "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
    "    response = json.loads(urllib.request.urlopen(url).read())\n",
    "    \n",
    "    result['result'].extend(response['itemListElement'])\n",
    "        \n",
    "with open(root + \"//fb.json\", 'w') as outfile:\n",
    "    json.dump(result, outfile, indent=4, sort_keys=True)"
   ]
  },
  {
   "source": [
    "### Retrieve missing from wikidata"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 13948/13948 [00:00<00:00, 1190810.78it/s]Missing: 593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "terms = set()\n",
    "missing = []\n",
    "\n",
    "jsonfile = load_json_file(root + \"//fb.json\")\n",
    "for x in tqdm(jsonfile[\"result\"]):\n",
    "    term = x[\"result\"][\"@id\"].replace(\"kg:\",\"\")\n",
    "    terms.add(term)\n",
    "\n",
    "for node in nodes:\n",
    "    if node not in terms:\n",
    "        missing.append(node)\n",
    "\n",
    "missing = [\"\\\"\" + x + \"\\\"\" for x in missing]\n",
    "print(\"Missing: \" + str(len(missing)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install sparqlwrapper\n",
    "# https://rdflib.github.io/sparqlwrapper/\n",
    "\n",
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "query = \"\"\"query=\n",
    "SELECT ?item ?code ?itemLabel ?itemDescription WHERE {\n",
    "  ?item wdt:P646 ?code.\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  VALUES ?code {\"\"\" + \" \".join(missing) + \"}}\"\n",
    "\n",
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0',\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "    'Accept': \"application/sparql-results+json\"\n",
    "}\n",
    "\n",
    "x = requests.post(endpoint_url, data = query, headers=headers)\n",
    "\n",
    "y = json.loads(x.text)\n",
    "with open(root + \"//fb_missing_wikidata_sparql.json\", 'w') as outfile:\n",
    "    json.dump(y, outfile, indent=4, sort_keys=True)"
   ]
  },
  {
   "source": [
    "Retrieve missing from https://github.com/yao8839836/kg-bert/blob/master/data/FB15K/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Create graph"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = Namespace(\"http://g.co/kg\")\n",
    "\n",
    "g = rdflib.Graph()\n",
    "\n",
    "g.bind(\"kg\", kg)\n",
    "g.bind(\"rdf\", RDF)\n",
    "g.bind(\"rdfs\", RDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 13948/13948 [00:03<00:00, 4219.00it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "missing = nodes.copy()\n",
    "\n",
    "jsonfile = load_json_file(root + \"//fb.json\")\n",
    "for x in tqdm(jsonfile[\"result\"]):\n",
    "    term = x[\"result\"][\"@id\"].replace(\"kg:\",\"\")\n",
    "    missing.remove(term)\n",
    "    g.add((\n",
    "        kg.term(term),\n",
    "        RDFS.label,\n",
    "        rdflib.Literal(x[\"result\"][\"name\"], datatype=XSD.string)\n",
    "\n",
    "    ))\n",
    "    if \"@type\" in x[\"result\"]:\n",
    "        for type_ in x[\"result\"][\"@type\"]:\n",
    "            g.add((\n",
    "                kg.term(term),\n",
    "                RDF.type,\n",
    "                rdflib.Literal(type_, datatype=XSD.string)\n",
    "            ))\n",
    "    if \"detailedDescription\" in x[\"result\"]:\n",
    "        g.add((\n",
    "            kg.term(term),\n",
    "            RDFS.comment,\n",
    "            rdflib.Literal(x[\"result\"][\"detailedDescription\"][\"articleBody\"], datatype=XSD.string)\n",
    "        ))\n",
    "    elif \"description\" in x[\"result\"]:\n",
    "        g.add((\n",
    "            kg.term(term),\n",
    "            RDFS.comment,\n",
    "            rdflib.Literal(x[\"result\"][\"description\"], datatype=XSD.string)\n",
    "        ))\n",
    "\n",
    "\n",
    "alt_labels = load_id_tsv(root + \"/FB15k_mid2name.txt\")\n",
    "alt_descriptions = load_id_tsv(root + \"/FB15k_mid2description.txt\")\n",
    "for entity in missing:\n",
    "    label = alt_labels.get(entity, None)\n",
    "    description = alt_descriptions.get(entity, \"\")\n",
    "    assert (label != None), \"Again missing \" + entity\n",
    "\n",
    "    g.add((\n",
    "            kg.term(entity),\n",
    "            RDFS.label,\n",
    "            rdflib.Literal(label.replace(\"_\", \" \"), datatype=XSD.string)\n",
    "\n",
    "        ))\n",
    "    g.add((\n",
    "        kg.term(entity),\n",
    "        RDFS.comment,\n",
    "        rdflib.Literal(description.replace(\"@en\",\"\").replace(\"\\\\n\", \"\").replace(\"\\\\\\\"\", \"\\\"\")[1:-1], datatype=XSD.string)\n",
    "\n",
    "    ))\n",
    "\n",
    "r\"\"\"\n",
    "missing = load_json_file(root + \"//fb_missing_wikidata_sparql.json\")\n",
    "p = re.compile('Q[1-9]+')\n",
    "for result in tqdm(missing[\"results\"][\"bindings\"]):\n",
    "    if(p.match(result[\"itemLabel\"][\"value\"]) == None):\n",
    "        term = result[\"code\"][\"value\"]\n",
    "        g.add((\n",
    "            kg.term(term),\n",
    "            RDFS.label,\n",
    "            rdflib.Literal(result[\"itemLabel\"][\"value\"], datatype=XSD.string)\n",
    "\n",
    "        ))\n",
    "        if \"itemDescription\" in result:\n",
    "            g.add((\n",
    "                kg.term(term),\n",
    "                RDFS.comment,\n",
    "                rdflib.Literal(result[\"itemDescription\"][\"value\"], datatype=XSD.string)\n",
    "\n",
    "            ))\n",
    "    else:\n",
    "        print(result[\"itemLabel\"][\"value\"])\n",
    "\"\"\"\n",
    "    \n",
    "g.serialize(os.path.abspath(r\"workspace/graphs/fb15k237.ttl\"),format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outfile = open(r\"workspace/graphs/fb15k237.ttl\", 'a')\n",
    "\n",
    "def read_set(path, typ):\n",
    "    content = None\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    for line in content:\n",
    "        head,rel,tail = line.split(\"\\t\")\n",
    "        outfile.write(f\"<<<http://g.co/kg{head}> <http://g.co/kg{rel}> <http://g.co/kg{tail}>>> obl:split obl:{typ} . \" + \"\\n\")\n",
    "\n",
    "read_set(train_path, 'train')\n",
    "read_set(test_path, 'test')\n",
    "read_set(valid_path, 'valid')\n",
    "\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "e7c37ba80a6e54a3d63188245ab5de6a3e0d381993bcb1990a7020536fc2299e"
   }
  },
  "interpreter": {
   "hash": "975f4df5516a56fef896b4876c749fefbbf20f04919a2b7836d7dfd2ac100f7a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}